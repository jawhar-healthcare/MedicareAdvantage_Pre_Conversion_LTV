{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6e2ec6-6303-4e0f-bfc4-5dc3bece1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5426433f-ccea-4192-8aa2-87387570d351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://hc-qa-mlflow-bucket', experiment_id='76', lifecycle_stage='active', name='Jawhar_notebook_test', tags={}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import arg\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from mlflow.exceptions import MlflowException\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "experiment_name = 'Jawhar_notebook_test'\n",
    "tracking_uri = 'https://mlflow.qa.healthcare.com/'\n",
    "\n",
    "# Tracking URL must be set before creating experiment, else the first run will be a local\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    print(f'Experiment {experiment_name} Not Found')\n",
    "    try:\n",
    "        mlflow.create_experiment(experiment_name,'s3://hc-qa-mlflow-bucket')\n",
    "    except MlflowException as ex:\n",
    "        print(f\"Error creating experiment {ex}\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfbf8f1-64ac-4313-9f08-6a5396a38947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ma_postconv_jorn_zcta_tu.csv',low_memory=False)\n",
    "# Define numeric and categorical features\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f6ebd1-a6f6-4f8a-a921-6a79f0a0756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for feature in categorical_columns:\n",
    "    df[feature] = pd.Series(df[feature], dtype=\"category\")\n",
    "\n",
    "df['all_LTV'] = df['mod_LTV'].fillna(0)\n",
    "y = df['all_LTV']\n",
    "X = df\n",
    "X = X.drop(['owner_phone',  'sk_referral_flag',  'lead_id', 'post_raw_cancellation_model_prediction',  'post_raw_probability_of_cancellation', 'post_raw_duration_model_prediction', 'post_raw_LTV',  'post_raw_coverage_duration', 'mod_LTV','all_LTV','application_id','owner_email', 'application_name','policy_id', 'owner_id', 'pol_zip_code', 'parent_application_id', 'bk_product_type', 'carrier', 'first_name', 'last_name', 'post_raw_application_id', 'post_raw_medicare_number', 'post_raw_policy_id', 'jrn_boberdoo_source','jrn_date'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X ,y , test_size=0.30)\n",
    "\n",
    "hyper_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': ['mae', 'rmse'],\n",
    "    'learning_rate': 0.005,\n",
    "    \"num_leaves\": 128,  \n",
    "    \"max_bin\": 512,\n",
    "}\n",
    "# iris = datasets.load_iris()\n",
    "# x = iris.data[:, 2:]\n",
    "# y = iris.target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "170fc28a-9125-4906-adbd-3eabd0aff8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jawhar/opt/anaconda3/envs/ma-dc/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31415\n",
      "[LightGBM] [Info] Number of data points in the train set: 14730, number of used features: 162\n",
      "[LightGBM] [Info] Start training from score 501.211307\n",
      "mae: 350.0647064649992\n",
      "median absolute error: 317.09425862488786\n",
      "r2: 0.021889441791020814\n",
      "RMSE :  372.322997\n",
      "138624.41419042094\n",
      "Experiment_id: 76\n",
      "Artifact Location: s3://hc-qa-mlflow-bucket\n",
      "Tags: {}\n",
      "Lifecycle_stage: active\n",
      "artifact_uri = s3://hc-qa-mlflow-bucket/e0917c710b4f4d678b613795f1a4d670/artifacts\n",
      "runID: e0917c710b4f4d678b613795f1a4d670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import numpy as np\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    estimator = random.randint(1, 11)\n",
    "    min_samples_leaf = random.randint(1, 5)\n",
    "    # params = {\n",
    "    #     \"n-estimators\": estimator,\n",
    "    #     \"min-samples-leaf\": 3,\n",
    "    #     \"features\": X_train.columns\n",
    "    # }\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Jawhar\")\n",
    "\n",
    "    # train the model\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    gbm = lgb.train(hyper_params, lgb_train, num_boost_round=10, verbose_eval=False)\n",
    "    predictions = gbm.predict(X_test)\n",
    "    \n",
    "    # rf = RandomForestRegressor(\n",
    "    #     n_estimators=estimator, min_samples_leaf=min_samples_leaf)\n",
    "    # rf.fit(X_train, y_train)\n",
    "    # predictions = rf.predict(X_test)\n",
    "\n",
    "    model_info = mlflow.sklearn.log_model(gbm, 'LGBM')\n",
    "    mlflow.log_params(hyper_params)\n",
    "\n",
    "    # log model performance\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f'mae: {mean_absolute_error(y_test, predictions)}')\n",
    "    print(f'median absolute error: {median_absolute_error(y_test, predictions)}')\n",
    "    print(f'r2: {r2_score(y_test, predictions)}')\n",
    "    rmse = np.sqrt(MSE(y_test, predictions))\n",
    "    print(\"RMSE : % f\" %(rmse))\n",
    "    \n",
    "    mlflow.log_metric('mse', mse)\n",
    "    mlflow.log_metric('rmse', rmse)\n",
    "    mlflow.log_metric('mae', mean_absolute_error(y_test, predictions))\n",
    "    mlflow.log_metric('median absolute error', median_absolute_error(y_test, predictions))\n",
    "    mlflow.log_metric('R2', r2_score(y_test, predictions))\n",
    "    print(mse)\n",
    "\n",
    "    # mlflow.log_artifact(\"testartifacts.txt\",artifact_path=mlflow.get_artifact_uri())\n",
    "\n",
    "    dictionary = {\"k\": \"v\"}\n",
    "    mlflow.log_dict(dictionary, \"data.json\")\n",
    "    # Log a dictionary as a YAML file in a subdirectory of the run's root artifact directory\n",
    "    mlflow.log_dict(dictionary, \"dir/data.yml\")\n",
    "\n",
    "    # If the file extension doesn't exist or match any of [\".json\", \".yaml\", \".yml\"],\n",
    "    # JSON format is used.\n",
    "    mlflow.log_dict(dictionary, \"data\")\n",
    "    mlflow.log_dict(dictionary, \"data.txt\")\n",
    "\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "    print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "    print(\"Tags: {}\".format(experiment.tags))\n",
    "    print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
    "    print(f'artifact_uri = {mlflow.get_artifact_uri()}')\n",
    "    print(f'runID: {run_id}')\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63392a77-d1a9-426f-86b9-edb994498908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
